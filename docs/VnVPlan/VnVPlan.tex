\documentclass[12pt, titlepage]{article}

\usepackage{longtable}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs,tabularx,makecell}
\usepackage{caption}
\usepackage{xcolor,colortbl}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Oct 17 2025 & 1.0 & Initial Draft\\
% Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

% \listoftables
% \wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{l l} 
%   \toprule		
%   \textbf{symbol} & \textbf{description}\\
%   \midrule 
%   T & Test\\
%   \bottomrule
% \end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}
\begin{table}[H]
\centering
\caption{Symbols, Abbreviations, and Acronyms}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\footnotesize

\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{Symbol / Acronym} & \textbf{Description} \\
\midrule
\arrayrulecolor[gray]{0.8}
API & Application Programming Interface – mechanism for data retrieval (e.g., GitHub API, PyPI API). \\
\hline
AHP & Analytic Hierarchy Process – method for pairwise comparison and ranking of libraries. \\
\hline
CSV & Comma-Separated Values – export format for datasets. \\
\hline
DB & Database – MySQL instance used for persistent data storage. \\
\hline
UI & User Interface – front-end component built with React. \\
\hline
NNL & Neural Network Libraries – the domain being analyzed (e.g., PyTorch, TensorFlow). \\
\hline
PoC & Proof of Concept – initial demonstration validating workflow integration. \\
\hline
VnV & Verification and Validation – process of ensuring correctness and meeting stakeholder needs. \\
\hline
CI/CD & Continuous Integration / Continuous Deployment – automated testing and deployment pipeline used in GitHub Actions. \\
\bottomrule
\end{tabularx}
\end{table}

\noindent\textit{All additional terms conform to those defined in the SRS Glossary (Section~4.1).}
\newpage

\pagenumbering{arabic}

% This document ... \wss{provide an introductory blurb and roadmap of the
%   Verification and Validation plan}
This document outlines the Verification and Validation (V\&V) strategy for the
Neural Network Libraries (NNL) Assessment Tool capstone project. It defines how
the team will confirm that the implemented system satisfies its specified
requirements, performs reliably, and aligns with the objectives stated in the
Software Requirements Specification (SRS), Development Plan, and Hazard
Analysis.

The V\&V Plan provides a structured roadmap covering requirement reviews,
system and nonfunctional testing, and traceability between test cases and
requirements. It also establishes the methods, responsibilities, tools, and
metrics that ensure all deliverables are verified for correctness and validated
against user and research expectations.

\section{General Information}
\label{sec:general-info}
\subsection{Summary}
\label{subsec:summary}
% \wss{Say what software is being tested.  Give its name and a brief overview of
%   its general functions.}
The Neural Network Libraries (NNL) Assessment Tool is a web-based application
that automates evidence collection, analysis, and visualization for assessing
open-source neural-network libraries such as PyTorch and TensorFlow.

The tool replaces spreadsheet-based scoring with a traceable, auditable system
that integrates the following automated features:

\begin{itemize}
  \item \textbf{Automated Data Collection:} retrieval of repository metrics
  (commits, issues, languages, and lines of code) through the GitHub API and
  PyPI metadata.

  \item \textbf{Interactive Data Table:} centralized, editable interface for
  entering and verifying measurements.

  \item \textbf{Automated Analytic Hierarchy Process (AHP):} automated
  computation of pairwise comparisons for reproducible rankings.

  \item \textbf{Visualization and Export:} generation of graphs and reports in
  PNG and \LaTeX{} formats for research use.
\end{itemize}

The V\&V Plan defines how the software’s correctness, reliability, and usability
will be verified and validated throughout its lifecycle.

\subsection{Objectives}
\label{subsec:objectives}
% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}
The objectives of this V\&V Plan are to:

\begin{enumerate}
  \item Build confidence in software correctness by verifying that each
  functional requirement in the SRS (e.g., automated AHP calculation, data
  retrieval, visualization, and multi-user collaboration) is fully implemented
  and traceable to its tests.

  \item Ensure system reliability and data integrity, particularly for automated
  data collection and storage operations identified as high-risk in the Hazard
  Analysis.

  \item Validate usability and accessibility through internal surveys and
  pilot-user feedback from the Research Sub-team and Dr.~Smith.

  \item Confirm performance and security non-functional requirements, such as
  load handling and access-control validation.
\end{enumerate}

\textbf{Out of Scope:}
\begin{itemize}
  \item Third-party API correctness (GitHub, PyPI); assumed verified by their
  respective providers.

  \item Formal proof techniques and hardware-level testing; outside academic
  scope.

  \item Multi-browser performance optimization beyond the core compatibility
  tests specified in the SRS (Chrome, Firefox, Safari, Edge).
\end{itemize}

\subsection{Challenge Level and Extras}
\label{subsec:challenge-level}
% \wss{State the challenge level (advanced, general, basic) for your project.
% Your challenge level should exactly match what is included in your problem
% statement.  This should be the challenge level agreed on between you and the
% course instructor.  You can use a pull request to update your challenge level
% (in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
% VnV planning exercise.}

% \wss{Summarize the extras (if any) that were tackled by this project.  Extras
% can include usability testing, code walkthroughs, user documentation, formal
% proof, GenderMag personas, Design Thinking, etc.  Extras should have already
% been approved by the course instructor as included in your problem statement.
% You can use a pull request to update your extras (in TeamComposition.csv or
% Repos.csv) if your plan changes as a result of the VnV planning exercise.}
\textbf{Challenge Level:} Advanced. Per the Problem Statement, the project
integrates a custom software tool that automates data gathering, analysis, and
visualization for neural-network libraries.

Although the broader capstone includes a research study on software-quality
assessment, the scope of this V\&V Plan pertains only to the verification and
validation of the software tool itself, not to the accompanying research
methodology or paper.


\textbf{Extras (Approved by Supervisor):} Usability Testing and Peer Code
Reviews, conducted through structured user surveys and GitHub-based peer
inspection checklists as defined in the Development Plan workflow.

\subsection{Relevant Documentation}
\label{subsec:relevant-docs}
% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.  You can create BibTeX entries for your
%   documents and within those entries include a hyperlink to the documents.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}
\begin{table}[H]
\centering
\caption{Relevant Documentation and Their Relevance to V\&V Activities}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\footnotesize

\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{Document} & \textbf{Relevance to V\&V Activities} \\
\midrule
\arrayrulecolor[gray]{0.8}
Software Requirements Specification (SRS) &
Defines functional and non-functional requirements that form the basis for
test derivation and traceability. \\
\hline
Development Plan &
Describes test environments, toolchains (PyTest, coverage.py, GitHub Actions),
and team responsibilities used for verification automation. \\
\hline
Hazard Analysis &
Identifies potential failures (e.g., data loss, access control errors, API
failures) that inform stress and reliability tests. \\
\hline
Problem Statement \& Goals &
Clarifies project scope, stakeholders, and intended outputs to align validation
activities with research objectives. \\
\hline
Design Document (MG/MIS – future) &
Will provide module interfaces and algorithms for unit test mapping in
Section~\ref{sec:unit-tests}. \\
\bottomrule
\end{tabularx}
\end{table}

\section{Plan}
\label{sec:plan}
% \wss{Introduce this section.  You can provide a roadmap of the sections to
%   come.}
This section defines the overall verification and validation (V\&V) strategy for
the Neural Network Libraries (NNL) Assessment Tool.

It outlines the V\&V team structure, review methods for each lifecycle artifact,
verification tools, and validation activities that will be conducted to ensure
functional and non-functional compliance with the SRS.

The plan follows both static reviews and dynamic execution-based testing,
combining automated pipelines with peer-driven inspections.

\subsection{Verification and Validation Team}
\label{subsec:vnv-team}
% \wss{Your teammates.  Maybe your supervisor.
%   You should do more than list names.  You should say what each person's role is
%   for the project's verification.  A table is a good way to summarize this information.}
\begin{table}[H]
\caption{Verification and Validation Team Responsibilities}
\begin{tabularx}{\textwidth}{l l >{\raggedright\arraybackslash}X}
\toprule
\textbf{Team Member} & \textbf{Role} & \textbf{Verification and Validation Responsibilities} \\
\midrule

Awurama Nyarko &
\textit{Research Lead / Usability \& Validation Analyst / Project Coordinator} &
Coordinates all V\&V activities and schedules artifact reviews (SRS, Design, VnV Plan). Maintains end-to-end traceability between requirements, hazards, and test cases. Leads usability validation (tasks, Likert survey) and documentation consistency checks. \\

\addlinespace[0.4em]

Fei &
\textit{Scrum Master / Developer / Automation Engineer} &
Owns CI automation for tests and reports (pytest, coverage, regression, performance jobs). Enforces Definition of Done for tests, monitors flakiness, and maintains regression suites and coverage dashboards. Facilitates stand-ups focused on verification risks and blockers. \\

\addlinespace[0.4em]

Haniye &
\textit{SRS Verification Lead / Backend Developer} &
Leads structured SRS inspections using the checklist; drives requirement clarifications and updates to the traceability matrix. Designs/implements unit and integration tests for backend data collection/validation and AHP computation paths. \\

\addlinespace[0.4em]

Ghena Hatoum &
\textit{QA Lead / Developer} &
Owns the defect log and test plan. Executes system tests for reliability/fault-tolerance and export integrity; verifies hazard mitigations (rollback, cache fallback, audit logging). Coordinates regression testing before milestones and reviews results. \\

\addlinespace[0.4em]

Dr.\ Spencer Smith &
\textit{External Reviewer / Supervisor} &
Provides independent reviews of SRS, Design, and VnV Plan; conducts milestone inspections (PoC, Rev0, Final) and gives acceptance feedback for verification completeness and validation quality. \\

\bottomrule
\end{tabularx}
\end{table}

\subsection{SRS Verification}
\label{subsec:srs-verification}
% \wss{List any approaches you intend to use for SRS verification.  This may
%   include ad hoc feedback from reviewers, like your classmates (like your
%   primary reviewer), or you may plan for something more rigorous/systematic.}

% \wss{If you have a supervisor for the project, you shouldn't just say they will
% read over the SRS.  You should explain your structured approach to the review.
% Will you have a meeting?  What will you present?  What questions will you ask?
% Will you give them instructions for a task-based inspection?  Will you use your
% issue tracker?}

% \wss{Maybe create an SRS checklist?}
Verification of the Software Requirements Specification (SRS) ensures that all
requirements are consistent, unambiguous, complete, and testable.

The team will use a combination of peer inspection, supervisor walkthrough, and
structured checklist verification to confirm requirement quality and
traceability.

Each requirement identified in the SRS (e.g., SR-AC1, SR-INT2, SR-IM4) will be
reviewed using a checklist based on the IEEE~29148 standard, evaluating criteria
such as correctness, completeness, consistency, verifiability, and
traceability.

During the scheduled peer inspection session, team members will mark any
ambiguous or unverifiable requirements for revision through the GitHub issue
tracker under the label ``SRS Review.''

Feedback from the assigned primary reviewer team will also be incorporated to
ensure external validation of requirement clarity.

Following the peer inspection, a structured 30-minute walkthrough will be
conducted with the supervisor.

In this meeting, the team will present the SRS structure, explain traceability
between requirement identifiers and their planned test cases, and clarify any
areas of uncertainty.

All decisions, clarifications, and follow-up actions from the session will be
logged in the repository’s issue tracker to maintain a transparent audit trail
of SRS verification outcomes.

\subsection{Design Verification}
\label{subsec:design-verification}
% \wss{Plans for design verification}
% \wss{The review will include reviews by your classmates}
% \wss{Create a checklists?}
The design verification process ensures that the software architecture and
module interfaces correctly realize all functional and non-functional
requirements defined in the SRS.

A structured design review will be conducted after completion of the Module
Guide (MG) and Module Interface Specification (MIS) documents.

The team will:

\begin{itemize}
  \item Perform checklist-based inspections of class diagrams, API schemas, and
  data-flow diagrams to confirm interface correctness, modularity, and
  traceability to SRS requirements.

  \item Use the issue tracker to document any inconsistencies or missing
  data-flow connections.

  \item Review database and API integration points to verify that the design
  addresses identified hazards such as data loss and synchronization errors.

  \item Hold a design walkthrough with the supervisor, during which reviewers
  will ask task-based questions (e.g., ``trace the path of a data update
  request'') to confirm the design supports required behaviour.
\end{itemize}

The checklist will evaluate design completeness, consistency, interface
clarity, and maintainability.

\subsection{Verification and Validation Plan Verification}
\label{subsec:vnv-plan-verification}
% \wss{The verification and validation plan is an artifact that should also be
% verified.  Techniques for this include review and mutation testing.}
% \wss{The review will include reviews by your classmates}
% \wss{Create a checklists?}

The V\&V Plan itself will be verified to ensure internal consistency and
feasibility.

Planned activities include:

\begin{itemize}
  \item \textbf{Peer review session:} each team member will inspect the document
  against the V\&V Plan template and rubric, checking for section completeness,
  alignment, and internal traceability.

  \item \textbf{Supervisor feedback:} Dr.~Smith will review the plan draft and
  provide comments prior to submission.

  \item \textbf{Version control:} changes and responses will be tracked in
  GitHub using pull-request comments and a ``VnV Plan Review'' label.

  \item \textbf{Mutation check:} minor edits (for example, changing requirement
  identifiers) will be tested to confirm that traceability references remain
  correct.
\end{itemize}

\subsection{Implementation Verification}
\label{subsec:implementation-verification}
% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}

% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}

% \wss{The final class presentation in CAS 741 could be used as a code
% walkthrough.  There is also a possibility of using the final presentation (in
% CAS741) for a partial usability survey.}
Implementation verification ensures that each software component is correctly
realized and adheres to design and coding standards.

\begin{itemize}
  \item \textbf{Static verification:} code reviews will be conducted for all pull
  requests. Linters (\texttt{flake8}, \texttt{black} for Python, \texttt{ESLint}
  for JavaScript) will enforce syntax and style compliance
  (see~\textit{Development Plan}, Section~2).

  \item \textbf{Dynamic verification:} unit and integration tests written in
  PyTest will confirm correct module behaviour and interface compatibility.

  \item \textbf{Continuous integration:} GitHub Actions will automatically
  execute all tests on commit and generate coverage reports (target $\geq$ 80\%).

  \item \textbf{Regression testing:} test suites will rerun automatically whenever
  code is merged to detect unexpected changes in functionality.

  \item \textbf{Code walkthroughs:} conducted at major milestones to compare
  implementation decisions with design diagrams and hazard mitigations.
\end{itemize}

\subsection{Automated Testing and Verification Tools}
\label{subsec:testing-tools}
% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

% \wss{If you have already done this in the development plan, you can point to
% that document.}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}
Automated verification will rely on the toolchain defined in the Development
Plan.

\begin{table}[H]
\centering
\caption{Automated Testing and Verification Tools}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\footnotesize

\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{Tool / Framework} & \textbf{Purpose} \\
\midrule
\arrayrulecolor[gray]{0.8}
PyTest + \texttt{coverage.py} &
Unit and integration testing with code-coverage metrics. \\
\hline
GitHub Actions CI/CD &
Executes automated tests and generates build reports for every push or pull
request. \\
\hline
\texttt{flake8} / \texttt{black} / \texttt{ESLint} &
Static analysis and style enforcement for Python and JavaScript. \\
\hline
Selenium or Playwright &
Automated front-end UI testing of key user workflows (domain creation and
visualization). \\
\hline
Postman or \texttt{pytest-requests} &
API endpoint validation and response-code checking. \\
\bottomrule
\end{tabularx}
\end{table}

Coverage summaries and logs will be archived in \texttt{/test/reports/} within
the repository.

\subsection{Software Validation}
\label{subsec:software-validation}

% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}

% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}

% \wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
% be used as an opportunity to validate the requirements.  You should plan on 
% demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
% The feedback from your supervisor will be very useful for improving your project.}

% \wss{For teams without an external supervisor, user testing can serve the same purpose 
% as a Rev 0 demo for the supervisor.}

% \wss{This section might reference back to the SRS verification section.}
Software validation confirms that the delivered NNL Assessment Tool satisfies
stakeholder expectations and performs reliably under normal operating
conditions.

Validation will combine functional scenario testing, usability evaluation, and
non-functional performance checks:

\begin{itemize}
  \item \textbf{End-to-end validation:} execute representative workflows, from
  data collection through AHP ranking and visualization, to confirm system
  integrity.

  \item \textbf{Usability assessment:} conducted with internal research users
  using short post-demo surveys (questions provided in Appendix~6.2).

  \item \textbf{Performance validation:} stress-test database operations and API
  response times to verify thresholds stated in SRS Section~4.3.

  \item \textbf{Security validation:} attempt invalid inputs and authentication
  edge cases to ensure protection against hazards such as data leakage and
  unauthorized access.

  \item \textbf{Supervisor review:} a final demonstration and feedback session
  with Dr.~Smith will confirm that the implemented system meets project
  objectives and research goals.
\end{itemize}

\section{System Tests}
\label{sec:system-tests}
% \wss{There should be text between all headings, even if it is just a roadmap of
% the contents of the subsections.}
Test IDs follow the pattern \texttt{T-XX-\#} where \texttt{XX} denotes the
subsystem (AC = Access Control, DM = Domain Management, VR = Visualization,
NF = Non-Functional).

\subsection{Tests for Functional Requirements}
\label{subsec:functional-tests}
% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.}

\subsection{Tests for Nonfunctional Requirements}

% \wss{The nonfunctional requirements for accuracy will likely just reference the
%   appropriate functional tests from above.  The test cases should mention
%   reporting the relative error for these tests.  Not all projects will
%   necessarily have nonfunctional requirements related to accuracy.}

% \wss{For some nonfunctional tests, you won't be setting a target threshold for
% passing the test, but rather describing the experiment you will do to measure
% the quality for different inputs.  For instance, you could measure speed versus
% the problem size.  The output of the test isn't pass/fail, but rather a summary
% table or graph.}

% \wss{Tests related to usability could include conducting a usability test and
%   survey.  The survey will be in the Appendix.}

% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
% format for the tests given below.}

% \wss{If you introduce static tests in your plan, you need to provide details.
% How will they be done?  In cases like code (or document) walkthroughs, who will
% be involved? Be specific.}


\subsection{Traceability Between Test Cases and Requirements}
\label{subsec:traceability}



\section{Unit Test Description}
N/a
% \wss{This section should not be filled in until after the MIS (detailed design
%   document) has been completed.}

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}  

% \wss{To save space and time, it may be an option to provide less detail in this section.  
% For the unit tests you can potentially layout your testing strategy here.  That is, you 
% can explain how tests will be selected for each module.  For instance, your test building 
% approach could be test cases for each access program, including one test for normal behaviour 
% and as many tests as needed for edge cases.  Rather than create the details of the input 
% and output here, you could point to the unit testing code.  For this to work, you code 
% needs to be well-documented, with meaningful names for all of the tests.}

% \subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 
					
% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 

% \item{...\\}
    
% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}
		
% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input/Condition: 
					
% Output/Result: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsubsection{Module ?}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}
\section{References}

\begin{enumerate}
  \item \textbf{V\&V Plan Template (McMaster Capstone, Dr.\ Spencer Smith).}\\
  Source template defining section structure, verification guidelines, and notes
  for functional/nonfunctional testing. Used as the basis for all section
  headings and content organization.

  \item \textbf{Software Requirements Specification (SRS) -- DomainX.}\\
  Source of all requirement identifiers (FR, LF, UH, PR, MS, SR). Referenced
  throughout Sections~3 and~4 for test case traceability.

  \item \textbf{Development Plan -- DomainX.}\\
  Referenced for verification workflow, CI/CD tools (\texttt{PyTest},
  \texttt{GitHub Actions}), and automation approach used in performance and
  maintainability testing.

  \item \textbf{Hazard Analysis -- DomainX.}\\
  Used for risk-based test design in reliability and security sections (API
  outage, rollback, and data-loss scenarios).

  \item \textbf{Problem Statement and Project Objectives -- DomainX.}\\
  Referenced for fit criteria in nonfunctional testing (accuracy $\pm$2\%,
  usability $\geq$ 4/5 score threshold) and for defining project scope
  (tool computation accuracy $\neq$ ML model accuracy).
\end{enumerate}

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters}
\label{sec:symbolic-parameters}
% The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
% Their values are defined in this section for easy maintenance.
The following symbolic parameters are referenced throughout this document and
may be updated centrally for consistency across all tests and scripts.

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Symbolic Constant} & \textbf{Description} & \textbf{Current Value / Note} \\ 
\midrule
\texttt{MAX\_LOGIN\_ATTEMPTS} & Number of consecutive failed login attempts before account lockout. & 5 \\
\texttt{AHP\_ERROR\_THRESHOLD} & Maximum acceptable relative error between manual and tool-calculated AHP scores. & 2\% \\
\texttt{USABILITY\_TARGET\_SCORE} & Minimum average usability rating required for success. & 4.0 / 5.0 \\
\texttt{PERF\_TEST\_DATASETS} & Dataset sizes used for performance benchmarking. & [5, 10, 20, 40 libraries] \\
\texttt{CI\_COVERAGE\_TARGET} & Minimum required automated test coverage. & $\geq$ 80\% \\
\bottomrule
\end{tabular}
\caption{Symbolic constants used in validation and verification tests.}
\end{table}

\subsection{Usability Survey Questions?}
\label{sec:usability-survey}
% \wss{This is a section that would be appropriate for some projects.}
These are the survey questions referenced in
Section~4.2.3~(Usability~Evaluation). Each question is rated on a 1–5
Likert scale (1 = Strongly Disagree, 5 = Strongly Agree).

\begin{enumerate}
  \item The interface layout was intuitive and easy to navigate.
  \item I could complete all main tasks (create domain, add libraries, export results) without external help.
  \item The information displayed in charts and tables was clear and easy to understand.
  \item The tool responded quickly to my actions (no major lag or errors).
  \item Error messages were clear and helped me understand how to fix issues.
  \item I found the overall design (colours, font size, contrast) visually comfortable.
  \item I would feel confident using this tool again for a research project.
\end{enumerate}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}