\documentclass{article}

\usepackage{tabularx}
\usepackage{booktabs}

\title{Problem Statement and Goals\\\progname}

\author{\authname}

\date{September 20, 2025}

\input{../Comments}
\input{../Common}

\begin{document}

\maketitle

\begin{table}[hp]
\caption{Revision History} \label{TblRevisionHistory}
\begin{tabularx}{\textwidth}{llX}
\toprule
\textbf{Date} & \textbf{Developer(s)} & \textbf{Change}\\
\midrule
September 20, 2025 & Awurama Nyarko, & Created first draft of document
 \\ & Fei Xie &
 \\ &Ghena Hatoum &
 \\ &Haniye Hamidizadeh &
 \\
September 28, 2025 & Awurama Nyarko & Expanded abbreviations on first use per peer review
 \\
\bottomrule
\end{tabularx}
\end{table}

\section{Problem Statement}

% \wss{You should check your problem statement with the
% \href{https://github.com/smiths/capTemplate/blob/main/docs/Checklists/ProbState-Checklist.pdf}
% {problem statement checklist}.} 

% \wss{You can change the section headings, as long as you include the required
% information.}

\subsection{Problem}
Open-source neural network (NN) libraries, including training frameworks
(e.g., PyTorch, TensorFlow), inference frameworks (e.g., ONNX Runtime),
and companion tooling (e.g., data loaders, visualization utilities), are central
to academic research and production AI systems. Despite their importance,
there is currently no transparent, reproducible, and domain-tailored way to
assess the state of software-development practice across these libraries.

Existing assessments are typically ad hoc: spreadsheet-based scoring and
manual pairwise comparisons (e.g., Analytic Hierarchy Process AHP) that are difficult to trace, with
evidence scattered across READMEs, documentation, tests, and continuous integration (CI)
pipelines. This creates challenges for:

\begin{enumerate}
  \item Users (researchers and engineers), who cannot easily compare libraries 
  on qualities such as installability, maintainability, understandability, usability, 
  transparency, robustness, reliability, and reproducibility.
  
  \item Maintainers, who lack cross-project feedback on practice gaps and 
  improvement opportunities.
  
  \item Researchers, who lack a consistent methodology and artifact trail 
  suitable for replication or meta-analysis.
\end{enumerate}

This project addresses these issues by adapting and applying a rigorous,
repeatable state-of-practice methodology to the NN-library domain, and
replacing fragile spreadsheets with a more traceable, auditable toolset that
streamlines evidence collection, scoring, and ranking. The outcome will be a
defensible comparison across software-engineering qualities and actionable
recommendations for practitioners and maintainers.


\subsection{Inputs and Outputs}
\paragraph{Inputs}
\begin{itemize}
  \item Curated set of open-source NN libraries that meet inclusion criteria
  (public repositories with source, installation docs, runnable examples/tutorials).
  \item Public artifacts: source code, documentation, tutorials, tests, CI configs,
  issue trackers, release notes.
  \item Repository measures: commit/issue activity, language breakdown,
  lines-of-code counts, collected via automated scripts.
  \item Measurement template responses: structured question bank per quality,
  completed via automation and human inspection.
  \item Decision-model configuration: weights over qualities (installability,
  maintainability, etc.) using a method such as AHP for reproducible rankings.
  \item Optional practitioner/maintainer interviews, if time permits.
\end{itemize}

\paragraph{Outputs}
\begin{itemize}
  \item Traceable dataset: structured table recording evidence and normalized
  scores per software quality.
  \item Rankings and sensitivity analyses: transparent results with full evidence trail,
  plus stability testing under weight changes.
  \item Domain insights: summarized patterns, practice gaps, correlations across projects.
  \item Recommendations: prioritized, evidence-backed suggestions for maintainers and users.
  \item Lightweight toolset: simple replacement for spreadsheets to collect evidence,
  apply scoring, and export results (comma-separated values (CSV)/PDF).
  \item Research deliverables: (i) an academic-style report documenting methods,
  dataset, findings; (ii) an open data/software package (versioned with changelog)
  for reproducibility and reuse.
\end{itemize}

% \wss{Characterize the problem in terms of ``high level'' inputs and outputs.  
% Use abstraction so that you can avoid details.}

\subsection{Stakeholders}
\begin{itemize}
  \item Supervisors and domain experts (primary): define scope, validate inclusion criteria,
  review methodology, and vet results.
  \item NN library maintainers and contributors (primary): benefit from diagnostics
  and recommendations; possible interview participants.
  \item Practitioners (researchers, engineers, data scientists) (primary): need trustworthy
  comparisons to select or advocate for libraries.
  \item Software engineering and RSE research community (secondary): gains reusable
  methodology, dataset, and toolset.
  \item Course instruction team (secondary): evaluates rigor, scope fit, and completeness
  of methodology and reporting.
\end{itemize}


\subsection{Environment}
\subsubsection{Hardware \& Execution Environment}

\begin{itemize}
    \item Storage: a solid-state drive (SSD) for fast data access
    \item Central processing unit (CPU): Multi-core processor to handle user requests
    \item Web Server: Used to store, process, deliver content
    \item Operating System: Linux to follow industry standards for web pages
\end{itemize}
\subsubsection{Software \& Tooling}
\begin{itemize}
    \item Frontend: HTML, CSS, JavaScript, and React, to build user interface
    \item Backend: Python and Django, to handle back-end logic and data processing
    \item Database: MySQL to store and manage data
    \item Application programming interfaces (APIs): e.g., GitHub API, to automatically fill in data
    \item Libraries: Data visualization and AHP libraries
    \item Version Control: Git to manage the project codebase
\end{itemize}

\paragraph{Assumptions}
\begin{itemize}
  \item Target libraries expose public repositories with source code.
  \item Installation documentation is available.
  \item Software qualities can be evaluated from surface artifacts within the project timeline.
  \item Stakeholders are available for periodic reviews.
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
  \item Fixed academic timeline.
  \item Limited graphics processing unit (GPU) access.
  \item Private or internal artifacts excluded.
  \item Interviews optional and subject to limited availability.
  \item Rapid upstream changes mitigated via timestamped data collection.
\end{itemize}

\paragraph{Out of Scope}
\begin{itemize}
  \item Benchmarking model accuracy.
  \item Benchmarking runtime or throughput performance.
  \item Domain-specific fitness evaluations beyond software-engineering qualities.
\end{itemize}


\section{Goals}
\subsection{Research}
At the end of our research we will have developed a research paper evaluate and understand the state of development practices within the NN libraries. Through this research paper we are aiming answer the questions mentioned in the Methodology for Assessing the State of the Practice for Domain X paper, which are as follows:
\begin{itemize}
    \item Development and Technical Stack: What are the common tools and methodologies used to build and manage these software packages?
    \item Developer Experience: What are the main challenges that developer face and how could they be resolved to improve development process and software quality?
    \item Quality and Best Practices: What actions are developers taking to ensure key software quality (e.g. usability, maintainability, and reproducibility)?
    \item Quality Comparison: How does a library's technical quality, as designed by this research methodology, compare to it's reputation within the broader developer community?
\end{itemize}
\subsection{Tool}
To enhance the efficiency of this research, we will develop a tool with several key features to minimize manual effort and improve data management.
\begin{itemize}
    \item \textbf{Interactive Data Table:} This feature will provide a visually appealing and intuitive table, allowing researchers to directly edit and manage the database with a user-friendly interface. \textbf{Fit Criterion:} At least 80\% of users (team and pilot testers) will rate the table $\geq$ 4 out of 5 in usability and visual clarity during internal evaluation.
    \item \textbf{Automated Data Collection:} The tool will automatically collect specific data points using Application Programming Interfaces (APIs) to reduce manual work and errors. \textbf{Fit Criterion:} Automation successfully retrieves and populates at least 90\% of targeted metrics (e.g., commits, issues) without human intervention.
    \item \textbf{Automated Analytic Hierarchy Process (AHP):} The tool will implement an automated AHP to compare libraries, guiding users through pairwise comparisons to weigh criteria and evaluate alternatives. \textbf{Fit Criterion:} Automated AHP results match expected manual calculations within $\pm$ 2\% variance for all tested scenarios.
    \item \textbf{Data Visualization and Download:} The tool will enable researchers to view and filter results as graphs and export them in formats such as PNG or LaTeX code. \textbf{Fit Criterion:} Visualizations correctly represent the underlying data and are exportable in all supported formats without error.
    \item \textbf{Collaboration:} The tool will allow domain experts to vet software lists and provide feedback while supporting simultaneous data entry by up to two users. \textbf{Fit Criterion:} Concurrent editing produces no data conflicts in 95\% of test cases and records accurate change logs.
\end{itemize}

\section{Stretch Goals}
The following are possible features that can be added to the tool however are not the focus.
\begin{itemize}
    \item Traceability: Currently, the use of Excel sheets makes it difficult to track who made what changes and when. Our tool will include a feature that logs all updates, creating a clear history of changes and improving the accountability of the database.
    \item Research Paper Generation: This feature will allow researchers to write up their findings directly on the tool. It will automatically compile important graphs and tables and format the final document in a user's preferred style, eliminating the need for manual formatting or direct use of LaTeX.
    \item Increase Library Features: While the current methodology focuses on evaluating development practices, we will expand our data collection to include information on a library's specific features and capabilities. This will add a new level of functionality, ensuring the tool can recommend a library that not only follows best practices but also aligns with a user's specific needs.
    \item Interactive Dashboard: This dashboard will provide a centralized, real-time overview of the project's progress. It will visually highlight key milestones, identify the next steps, and clearly show any missing data or required feedback from domain experts.
\end{itemize}
\section{Extras}
Following a discussion with our supervisor, Dr. Smith, it confirmed that a research paper will serve as our extra and that there will be no need to have another.
For more information on the paper's scope and purpose, please refer to Section 2.1 Research.
% \wss{For CAS 741: State whether the project is a research project. This
% designation, with the approval (or request) of the instructor, can be modified
% over the course of the term.}

% \wss{For SE Capstone: List your extras.  Potential extras include usability
% testing, code walkthroughs, user documentation, formal proof, GenderMag
% personas, Design Thinking, etc.  (The full list is on the course outline and in
% Lecture 02.) Normally the number of extras will be two.  Approval of the extras
% will be part of the discussion with the instructor for approving the project.
% The extras, with the approval (or request) of the instructor, can be modified
% over the course of the term.}
\begin{itemize}
  \item An academic-style report documenting the methodology, dataset, and findings.
\end{itemize}

\newpage{}

\section*{Appendix --- Reflection}

% \wss{Not required for CAS 741}

% \input{../Reflection.tex}

\begin{enumerate}
    \item What went well while writing this deliverable? \\
    \textbf{Awurama:} I found that structuring the problem into the four subsections (Problem, Inputs/Outputs, Stakeholders, Environment) went smoothly. Having my draft already written in a google doc made it much easier to adapt to LaTeX. 
    \\\textbf{Ghena:} I was able to shift my perspective on the problem after having a meeting with our supervisor, which lead to a more accurate understanding of the PS and goals.
    \\\textbf{Fei:} While writing the deliverable, we were able to as a team have a deeper understanding of what this project is about, and solidify what our own expectations were for this project, through the goals determined and problem statement.
    \\\textbf{Haniye:} Working together on the goals and problem statement helped us see the project from different angles. By sharing our perspectives, we were able to clarify the goals and cover more aspects of the problem than we would have on our own.

    \item What pain points did you experience during this deliverable, and how
    did you resolve them? \\
    \textbf{Awurama:} My main challenge was getting familiar with the LaTeX template and re-adjusting to the GitHub workflow after not using it for a while. I overcame this by researching LaTeX best practices and following the problem statement writing checklist to make sure I stayed on track.  
    \\\textbf{Ghena:} Initially, my understanding of the problem statement was centered on the operational challenges faced by researchers, rather than the foundational importance of the research itself. A clarifying discussion with Dr. Smith provided the crucial insight needed to reframe my thinking, enabling me to grasp the project's broader significance. This adjustment allowed me to understand the problem statement and goals that was much more aligned with the supervisor's expectations and the project's core purpose. Need to get better at taking meeting notes.
    \\\textbf{Fei:} Due to the two parts of this project, some members of the team took longer to understand the research side, which affect the understanding of what is required for the tool (i.e. what does the data look like). While writing this deliverable, we were able to identify the aspects that weren’t as clear, and schedule meetings with our supervisor to clarify it for everyone. 
    \\\textbf{Haniye:} In the beginning it was a bit confusing to figure out what exactly we should write in the problem statement since the project has both a research side and a tool side. We sometimes wrote a part and then realized we needed to change it as new ideas came up. Meeting with our supervisor really helped us clear the confusion and know how to move forward.

    \item How did you and your team adjust the scope of your goals to ensure
    they are suitable for a Capstone project (not overly ambitious but also of
    appropriate complexity for a senior design project)? \\
    \textbf{Awurama:} As a team, we agreed to keep the project focused on assessing software-engineering practices of neural network libraries, and to exclude accuracy and performance benchmarking. This adjustment kept the scope realistic for the Capstone timeline and ensured it matched the course expectations.  
    \\\textbf{Ghena:} Through our meeting with our supervisor, we were able to adjust the scope of our goals to ensure they are suitable for a Capstone project. We discussed what the expected results are and what would be possible add-ons is we had time.
    \\\textbf{Fei:} By interacting with our supervisor early on in the process about what the team though the expectations were, we were able to quickly narrow down the focus, levaging the expertise of our supervisor on their past expertise of overseeing numerous past capstone projects. Identifying what were the base functionality compared to “nice-to-have’s” were also important. That way we can ensure we at least finish the project, instead of being stuck in development trying to implement something that is just a cool feature
    \\\textbf{Haniye:} Talking with our supervisor and sharing our ideas helped us figure out what was most important to focus on and what we could leave out for later. It made it easier to set realistic goals for the Capstone timeline. Our past group project experience also helped us know how to keep things practical.

\end{enumerate}

\end{document}