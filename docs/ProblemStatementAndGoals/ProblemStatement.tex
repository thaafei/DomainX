\documentclass{article}

\usepackage{tabularx}
\usepackage{booktabs}

\title{Problem Statement and Goals\\\progname}

\author{\authname}

\date{September 20, 2025}

\input{../Comments}
\input{../Common}

\begin{document}

\maketitle

\begin{table}[hp]
\caption{Revision History} \label{TblRevisionHistory}
\begin{tabularx}{\textwidth}{llX}
\toprule
\textbf{Date} & \textbf{Developer(s)} & \textbf{Change}\\
\midrule
    2025-09-20 & Awurama Nyarko, Fiona Xie, Ghena Hatoum, Fatemeh Hamidizadeh & Initial draft of Problem Statement (Problem, Inputs/Outputs, Stakeholders, Environment, Goals, Stretch, Extras)\\
\bottomrule
\end{tabularx}
\end{table}

\section{Problem Statement}

% \wss{You should check your problem statement with the
% \href{https://github.com/smiths/capTemplate/blob/main/docs/Checklists/ProbState-Checklist.pdf}
% {problem statement checklist}.} 

% \wss{You can change the section headings, as long as you include the required
% information.}

\subsection{Problem}
Open-source neural network (NN) libraries, including training frameworks
(e.g., PyTorch, TensorFlow), inference frameworks (e.g., ONNX Runtime),
and companion tooling (e.g., data loaders, visualization utilities), are central
to academic research and production AI systems. Despite their importance,
there is currently no transparent, reproducible, and domain-tailored way to
assess the state of software-development practice across these libraries.

Existing assessments are typically ad hoc: spreadsheet-based scoring and
manual pairwise comparisons (e.g., AHP) that are difficult to trace, with
evidence scattered across READMEs, documentation, tests, and CI
pipelines. This creates challenges for:

\begin{enumerate}
  \item Users (researchers and engineers), who cannot easily compare libraries 
  on qualities such as installability, maintainability, understandability, usability, 
  transparency, robustness, reliability, and reproducibility.
  
  \item Maintainers, who lack cross-project feedback on practice gaps and 
  improvement opportunities.
  
  \item Researchers, who lack a consistent methodology and artifact trail 
  suitable for replication or meta-analysis.
\end{enumerate}

This project addresses these issues by adapting and applying a rigorous,
repeatable state-of-practice methodology to the NN-library domain, and
replacing fragile spreadsheets with a more traceable, auditable toolset that
streamlines evidence collection, scoring, and ranking. The outcome will be a
defensible comparison across software-engineering qualities and actionable
recommendations for practitioners and maintainers.


\subsection{Inputs and Outputs}
\paragraph{Inputs}
\begin{itemize}
  \item Curated set of open-source NN libraries that meet inclusion criteria
  (public repositories with source, installation docs, runnable examples/tutorials).
  \item Public artifacts: source code, documentation, tutorials, tests, CI configs,
  issue trackers, release notes.
  \item Repository measures: commit/issue activity, language breakdown,
  lines-of-code counts, collected via automated scripts.
  \item Measurement template responses: structured question bank per quality,
  completed via automation and human inspection.
  \item Decision-model configuration: weights over qualities (installability,
  maintainability, etc.) using a method such as AHP for reproducible rankings.
  \item Optional practitioner/maintainer interviews, if time permits.
\end{itemize}

\paragraph{Outputs}
\begin{itemize}
  \item Traceable dataset: structured table recording evidence and normalized
  scores per software quality.
  \item Rankings and sensitivity analyses: transparent results with full evidence trail,
  plus stability testing under weight changes.
  \item Domain insights: summarized patterns, practice gaps, correlations across projects.
  \item Recommendations: prioritized, evidence-backed suggestions for maintainers and users.
  \item Lightweight toolset: simple replacement for spreadsheets to collect evidence,
  apply scoring, and export results (CSV/PDF).
  \item Research deliverables: (i) an academic-style report documenting methods,
  dataset, findings; (ii) an open data/software package (versioned with changelog)
  for reproducibility and reuse.
\end{itemize}

% \wss{Characterize the problem in terms of ``high level'' inputs and outputs.  
% Use abstraction so that you can avoid details.}

\subsection{Stakeholders}
\begin{itemize}
  \item Supervisors and domain experts (primary): define scope, validate inclusion criteria,
  review methodology, and vet results.
  \item NN library maintainers and contributors (primary): benefit from diagnostics
  and recommendations; possible interview participants.
  \item Practitioners (researchers, engineers, data scientists) (primary): need trustworthy
  comparisons to select or advocate for libraries.
  \item Software engineering and RSE research community (secondary): gains reusable
  methodology, dataset, and toolset.
  \item Course instruction team (secondary): evaluates rigor, scope fit, and completeness
  of methodology and reporting.
\end{itemize}


\subsection{Environment}
\subsection{Hardware \& Execution Environment}

\begin{itemize}
    \item Storage: A SSD for fast data access
    \item CPU: Multi-core processor to handle user requests
    \item Web Server: Used to store, process, deliver content
    \item Operating System: Linux to follow industry standards for web pages
\end{itemize}
\subsection{Software \& Tooling}
\begin{itemize}
    \item Frontend: HTML, CSS, JavaScript, and React, to build user interface
    \item Backend: Python and Django, to handle back-end logic and data processing
    \item Database: MySQL to store and manage data
    \item APIs: GitHub API, to automatically fill in data
    \item Libraries: Data visualization and AHP libraries
    \item Version Control: Git to manage the project codebase
\end{itemize}

\paragraph{Assumptions}
\begin{itemize}
  \item Target libraries expose public repositories with source code.
  \item Installation documentation is available.
  \item Software qualities can be evaluated from surface artifacts within the project timeline.
  \item Stakeholders are available for periodic reviews.
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
  \item Fixed academic timeline.
  \item Limited GPU access.
  \item Private or internal artifacts excluded.
  \item Interviews optional and subject to limited availability.
  \item Rapid upstream changes mitigated via timestamped data collection.
\end{itemize}

\paragraph{Out of Scope}
\begin{itemize}
  \item Benchmarking model accuracy.
  \item Benchmarking runtime or throughput performance.
  \item Domain-specific fitness evaluations beyond software-engineering qualities.
\end{itemize}


\section{Goals}
\subsection{Research}
At the end of our research we will have developed a research paper evaluate and understand the state of development practices within the NN libraries. Through this research paper we are aiming answer the questions mentioned in the Methodology for Assessing the State of the Practice for Domain X paper, which are as follows:
\begin{itemize}
    \item Development and Technical Stack: What are the common tools and methodologies used to build and manage these software packages?
    \item Developer Experience: What are the main challenges that developer face and how could they be resolved to improve development process and software quality?
    \item Quality and Best Practices: What actions are developers taking to ensure key software quality (e.g. usability, maintainability, and reproducibility)?
    \item Quality Comparison: How does a library's technical quality, as designed by this research methodology, compare to it's reputation within the broader developer community?
\end{itemize}
\subsection{Web Application}
To enhance the efficiency of this research, we will develop a web application with several key features to minimize manual effort and improve data management.
\begin{itemize}
    \item Interactive Data Table: This feature will provide a visually appealing and intuitive web-based table, allowing researchers to directly edit and manage the database with a user-friendly interface.
    \item Automated Data Collection: We will automate the collection of specific data points using APIs to reduce manual work and the chance of errors. While some metrics like installability and sustainability will still require manual expert ranking, the tool will automatically gather data such as number of commits and issue counts.
    \item Automate Analytical Hierarchy Process: The web app will implement an automated AHP on the web app to compare libraries, guiding users through a series of pairwise comparisons to weigh criteria and evaluate libraries against each other.
    \item Data Visualization and Download: The tool will enable researchers to view and filter results as graphs. Users will be able to download these graphs in various formats, including PNG files for presentations and LaTeX code for direct use in papers. It will also allow researchers to download the entire database for any scope as either an Excel or JSON file, providing greater flexibility and control over the data.
    \item Collaboration: This methodology requires close collaboration between software and domain experts. The tool will facilitate this by allowing domain experts to vet software lists and provide feedback on the results. To support simultaneous data entry, the user interface will be designed to allow up to two people to update the database concurrently.
\end{itemize}

\section{Stretch Goals}
The following are possible features that can be added to the web app however are not the focus.
\begin{itemize}
    \item Traceability: Currently, the use of Excel sheets makes it difficult to track who made what changes and when. Our web app will include a feature that logs all updates, creating a clear history of changes and improving the accountability of the database.
    \item Research Paper Generation: This feature will allow researchers to write up their findings directly on the web app. It will automatically compile important graphs and tables and format the final document in a user's preferred style, eliminating the need for manual formatting or direct use of LaTeX.
    \item Increase Library Features: While the current methodology focuses on evaluating development practices, we will expand our data collection to include information on a library's specific features and capabilities. This will add a new level of functionality, ensuring the tool can recommend a library that not only follows best practices but also aligns with a user's specific needs.
    \item Interactive Dashboard: This dashboard will provide a centralized, real-time overview of the project's progress. It will visually highlight key milestones, identify the next steps, and clearly show any missing data or required feedback from domain experts.
\end{itemize}
\section{Extras}
Following a discussion with our supervisor, Dr. Smith, it confirmed that a research paper will serve as our extra and that there will be no need to have another.
For more information on the paper's scope and purpose, please refer to Section 2.1 Research.
% \wss{For CAS 741: State whether the project is a research project. This
% designation, with the approval (or request) of the instructor, can be modified
% over the course of the term.}

% \wss{For SE Capstone: List your extras.  Potential extras include usability
% testing, code walkthroughs, user documentation, formal proof, GenderMag
% personas, Design Thinking, etc.  (The full list is on the course outline and in
% Lecture 02.) Normally the number of extras will be two.  Approval of the extras
% will be part of the discussion with the instructor for approving the project.
% The extras, with the approval (or request) of the instructor, can be modified
% over the course of the term.}
\begin{itemize}
  \item An academic-style report documenting the methodology, dataset, and findings.
\end{itemize}

\newpage{}

\section*{Appendix --- Reflection}

% \wss{Not required for CAS 741}

% \input{../Reflection.tex}

\begin{enumerate}
    \item What went well while writing this deliverable? 
    
    \textbf{Awurama:} I found that structuring the problem into the four subsections 
    (Problem, Inputs/Outputs, Stakeholders, Environment) went smoothly. Having my 
    draft already written in a google doc made it much easier to adapt to LaTeX. 
    \textbf{Ghena:} I was able to shift my perspective on the problem after having a meeting with our supervisor, which lead to a more accurate understanding of the PS and goals.
    \textbf{Fei:} While writing the deliverable, we were able to as a team have a deeper understanding of what this project is about, and solidify what our own expectations were for this project, through the goals determined and problem statement.

    \item What pain points did you experience during this deliverable, and how
    did you resolve them?

    \textbf{Awurama:} My main challenge was getting familiar with the LaTeX template 
    and re-adjusting to the GitHub workflow after not using it for a while. I overcame this 
    by researching LaTeX best practices and following the problem statement writing checklist 
    to make sure I stayed on track.  
    \textbf{Ghena:} Initially, my understanding of the problem statement was centered on the operational challenges faced by researchers, rather than the foundational importance of the research itself. A clarifying discussion with Dr. Smith provided the crucial insight needed to reframe my thinking, enabling me to grasp the project's broader significance. This adjustment allowed me to understand the problem statement and goals that was much more aligned with the supervisor's expectations and the project's core purpose. Need to get better at taking meeting notes.
    \textbf{Fei:} Due to the two parts of this project, some members of the team took longer to understand the research side, which affect the understanding of what is required for the tool (i.e. what does the data look like). While writing this deliverable, we were able to identify the aspects that weren’t as clear, and schedule meetings with our supervisor to clarify it for everyone. 
    
    \item How did you and your team adjust the scope of your goals to ensure
    they are suitable for a Capstone project (not overly ambitious but also of
    appropriate complexity for a senior design project)?

    \textbf{Awurama:} As a team, we agreed to keep the project focused on assessing 
    software-engineering practices of neural network libraries, and to exclude accuracy 
    and performance benchmarking. This adjustment kept the scope realistic for the 
    Capstone timeline and ensured it matched the course expectations.  
    \textbf{Ghena:} Through our meeting with our supervisor, we were able to adjust the scope of our goals to ensure they are suitable for a Capstone project. We discussed what the expected results are and what would be possible add-ons is we had time.
    \textbf{Fei:} By interacting with our supervisor early on in the process about what the team though the expectations were, we were able to quickly narrow down the focus, levaging the expertise of our supervisor on their past expertise of overseeing numerous past capstone projects. Identifying what were the base functionality compared to “nice-to-have’s” were also important. That way we can ensure we at least finish the project, instead of being stuck in development trying to implement something that is just a cool feature
\end{enumerate}  

\end{document}